Rudra â€” Intelligent Voice Assistant

Rudra is a modular, Python-based intelligent voice assistant designed to run reliably on Linux systems.
The project is built step-by-step with a strong focus on architecture, stability, and extensibility, rather than quick features.

Current implementation uses Google Speech Recognition for accurate voice input, with a long-term goal of becoming an offline-first, algorithm-driven assistant.

ğŸ“Œ Project Status

Current stable milestone: âœ… Day 9 (Stable)

Day 9 focuses entirely on input intelligence, stability, and conversational reliability.

âœ” Robust input validation
âœ” Confidence-based intent gating
âœ” Active listening with silence handling
âœ” Repeat-safe retry logic
âœ” Stable conversation loop

Action-based system control begins from Day 10.

ğŸš€ Features (Implemented)
âœ… Core Assistant

Intent-based command processing

Modular NLP pipeline

Short-term & long-term memory

MySQL-backed persistent storage

Clean separation of concerns

âœ… Input System (Day 8)

Voice input using Google Speech Recognition

Text input fallback

Push-to-talk (press ENTER to speak)

Configurable input mode (voice / text)

Controlled listening (no always-on mic)

âœ… Input Intelligence (Day 9)

Input normalization & validation

Rejection of weak, noisy, or partial input

Confidence scoring & refinement

Safe retry handling (no accidental blocking)

Explicit handling of unknown intents

âœ… Active Listening (Day 9)

Listening states: idle, active, waiting

Silence-aware behavior

Automatic sleep on repeated silence

Natural conversation flow (no mic lock)

âœ… Stability & Logging

Structured logging with Loguru

Graceful handling of speech errors

Environment-variable based configuration

Safe .env usage (never committed)

ğŸ§  Architecture Overview

Rudra follows a layered, deterministic pipeline:

Input
 â†“
Normalization & Validation
 â†“
Tokenization
 â†“
Intent Scoring
 â†“
Confidence Refinement
 â†“
Skill Execution
 â†“
Memory Update


Each layer is independent, testable, and replaceable.

ğŸ—‚ï¸ Project Structure
core/
â”œâ”€â”€ assistant.py            # Main assistant loop
â”œâ”€â”€ main.py                 # Entry point
â”œâ”€â”€ config.py               # Input configuration
â”œâ”€â”€ input_controller.py     # Centralized input handling
â”‚
â”œâ”€â”€ input/
â”‚ â””â”€â”€ input_validator.py    # Input validation & retry logic
â”‚
â”œâ”€â”€ speech/
â”‚ â””â”€â”€ google_engine.py      # Google Speech Recognition
â”‚
â”œâ”€â”€ nlp/
â”‚ â”œâ”€â”€ tokenizer.py
â”‚ â”œâ”€â”€ normalizer.py
â”‚ â””â”€â”€ intent.py
â”‚
â”œâ”€â”€ intelligence/
â”‚ â”œâ”€â”€ intent_scorer.py
â”‚ â””â”€â”€ confidence_refiner.py
â”‚
â”œâ”€â”€ skills/
â”‚ â””â”€â”€ basic.py
â”‚
â”œâ”€â”€ context/
â”‚ â”œâ”€â”€ short_term.py
â”‚ â””â”€â”€ long_term.py
â”‚
â”œâ”€â”€ storage/
â”‚ â”œâ”€â”€ mysql.py
â”‚ â””â”€â”€ models.py

â–¶ï¸ How to Run
Requirements

Python 3.10+

MySQL (running locally)

Linux OS

Working microphone

Run Command
python3 -m core.main

ğŸ› ï¸ Configuration

Sensitive values are stored in .env

.env is never committed

MySQL credentials and speech settings are configurable

ğŸ§­ Roadmap
Upcoming

Day 10 â€” Action-based intents (system commands)

Day 11 â€” Contextual multi-step commands

Day 12+ â€” Memory intelligence improvements

Long-Term Vision

Offline-first intelligence

Algorithmic & ML-based intent engine

Multi-device sync

Android & Raspberry Pi support

Alexa/Siri-class assistant behavior

ğŸ·ï¸ Milestones

day-8-stable â€” Input system & voice pipeline

day-9-stable â€” Input intelligence & active listening

ğŸ“„ License

This project is under active development and intended for learning, research, and portfolio use.